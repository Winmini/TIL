# 머신러닝, 딥러닝

## - Tensorflow



사이킷런으로만 모든 문제를 해결하기엔 답이 없어서 어느정도는 구현해야한다. 순수 파이썬만 하기엔 너무 버겁고 해도 속도가 너무느리며, 사이킷런으로도 하기엔 한계가 생긴다. 그래서 구현도 해야하고 라이브러리도 사용해야 한다. 그 라이브러리중 가장 유명한게 Facebook의 파이토치와 Google의 Tensorflow였다. 둘다 장단점이 있으며, 괜찮은 언어이다. 그 중 Tensorflow에 대해서 배워볼까 한다.

Tensorflow는 1.0버전과 2.0버전이 있다. 1.0버전은 머신러닝에 대해 어느정도 알고 있다면 변형해서 하기 좋고 2.0버전은 좀 더 사용자가 편하게 만들어놓은 버전이라서 어떻게 수정하기가 힘들다. 

따라서 둘다 해볼 예정이며, 1.0이 개발자에겐 좀 더 좋을 수 있다.

### Tensorflow란?

- open source software library이다. 무료로 사용할 수 있다.
- 수치적 계산을 하기 위해 사용한다.
- data flow graphs를 이용한다. (Node와 Edge로 구성된 방향성있는 그래프)
  - Node는 numerical operation(수치연산)과 데이터의 입출력을 담당한다.
  - Node는 여러개 있을 수 있고 서로 연결되어 있을 수도 있다. 이 연결선을 Edge라 한다.
  - Node의 데이터는 Edge로 연결된 곳으로만 데이터를 줄 수 있으며 Edge의 방향인 한쪽방향으로만 가능하다.
  - Edge를 통해 흘러가는 Data를 <u>Tensor</u>(동적 크기의 다차원 배열)라 한다. 
  - 따라서 Node에는 Tensor가 존재한다.

Tensorflow는 1.0과 2.0버전의 변화가 너무 커서 호환성이 없다. 보통 상위버전이 하위버전을 호환하게 만들지만 Tensorflow는 아니다. 내부적으로는 많이 변했지만, 겉으로 보기엔 별차이 없어보이기도 한다. 머신러닝 이론을 따라가면서 구현할 수 있는 1.0버전부터 즉, 1.15버전으로 해보자.

코랩은 기본적으로 2버전이므로 주피터 노트북으로 마저하자

`conda activate data_env`로 가서 `conda install tensorflow==1.15`로 설치하고 jupyter notebook을 실행해보자.

---

### 실습

```python
import tensorflow as tf

print(tf.__version__)
# 버전 체크

node = tf.constant('Hello World')
print(node)
# 이렇게 출력하는게 아니다.
# 그래프를 실행하려면 session이 필요하다.

sess = tf.Session()
sess.run(node)
print(sess.run(node).decode())
-----------------------------------------------------------------------------
1.15.0
Tensor("Const:0", shape=(), dtype=string)
b'Hello World' # byte 배열의 약자
Hello World
```

이렇게 node를 만들고 session을 통해 만든다. 그림을 통해 이해해보자.

![그래프](./jpgfile/node.png)

이 그래프를 실습해보자.

```python
import tensorflow as tf


node1 = tf.constant(10, dtype=tf.float32)
node2 = tf.constant(20, dtype=tf.float32)
node3 = node1 + node2

sess = tf.Session()

print(sess.run(node3))
# node3만 실행시켜도 아래쪽 노드도 같이 실행이 된다.
print(sess.run([node3, node1]))
# 노드를 같이 실행할 수 있다.
------------------------------------------------------------------------------
30.0
[30.0, 10.0]
```

상수 10과 20을 이용했지만, 변수를 이용하려면 어떻게 해야할까?

```python
import tensorflow as tf

node1 = tf.placeholder(dtype=tf.float32)
node2 = tf.placeholder(dtype=tf.float32)
# 데이터가 없고 데이터를 보관할 장소만 있는것, 어떤타입이 들어오는지는 지정가능
node3 = node1 + node2

sess = tf.Session()

print(sess.run(node3, feed_dict={node1: 50, node2: 100}))
# 이렇게 먹이를 줘서 실행시켜야 가능하다.
------------------------------------------------------------------------------
150.0
```

여기까지가 tensorflow의 아주 기본적인 사용법이다. 이를 이용해 Multiple Linear Regression을 구현해보자. 저번에 했던 data를 가지고 머신러닝코드를 작성해보자.

텐서플로를 쓰더라도 이상치, 결측치, 정규화같은 데이터 전처리작업은 무조건 이뤄져야 하는것이다. 텐서플로는 머신러닝코드를 구현하기 위함이다.

잘 구현되는지를 위해 싸이킷런과 비교해보자.

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy import stats
from sklearn import linear_model
from sklearn.preprocessing import MinMaxScaler

# RAW DATA
df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Solar.R', 'Wind', 'Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')

# 2. 이상치(outlier) 처리, 지대점은 그냥 주자
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]

# 3. 정규화 - Min-Max Normalization
scaler_x = MinMaxScaler()
# 입력값 전체를 담당할 것이므로 여전히 1개만 만든다.
scaler_y = MinMaxScaler()

scaler_x.fit(training_data.iloc[:,:-1].values)  
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))   

training_data.iloc[:,:-1] = scaler_x.transform(training_data.iloc[:,:-1].values)
training_data['Ozone'] = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))

# Training Data Set
x_data = training_data.iloc[:,:-1].values
t_data = training_data['Ozone'].values.reshape(-1,1)

# Tensorflow

# 1. placeholder
x = tf.placeholder(shape=[None,3], dtype=ft.float32)
y = tf.placeholder(shape=[None,1], dtype=ft.float32)
'''
입력값 전체가 여기로 들어간다. 단순 스칼라가 아니라 차원이 존재하면 반드시 명시해야함
우리의 데이터는 컬럼수는 고정되어있지만 행은 고정되어있지 않다. 당장이야 110개지만,
그렇다고 110이라고 줘버리면 우리가 110번넣어야 한다.
들어오는 데이터에 대해서 신경쓰지 않는다의 의미로 None을 쓴다.
None이 없다는 의미가 아니다.
'''

# 2. Weight & bias
W = tf.Variable(tf.random.normal([3,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

# 3. Multiple Linear Regression Model => Hypothesis(가설)
H = tf.matmul(X, W) + b
# H는 새로운 Node가 된 것이다.

# 4. loss function
loss = tf.reduce_mean(tf.square(H-T))

# 5. 학습
# 1번 편미분해서 w와 b를 갱신하는 역할을 하는 node를 생성
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)
# 편미분해서 했던 작업들 이 옵티마이저가 다 해줌, 그렇게해서 딱 '한번' 갱신

# 6. Session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 초기화
# 변수가 있으면 반드시 해줘야하는 초기화작업이지만, 어떻게 진행되는지
# 의미가 있는지 잘모르겠다. 그래서 2버전부턴 안해도된다.

for step in range(300000):
    tmp, W_val, b_val, loss_val = sess.run([train, W, b, loss],
                                          feed_dict={X:x_data,
                                                     T:t_data})
    if step%30000 ==0:
        print('W:{}, b:{}, loss:{}'.format(W_val, b_val, loss_val))
------------------------------------------------------------------------------
# ERROR는 무시하자.
W:[[-1.7595997 ]
 [ 1.3001919 ]
 [ 0.87295866]], b:[2.003274], loss:3.232383966445923
W:[[-1.4751933 ]
 [ 0.46871403]
 [ 0.60989165]], b:[0.7033175], loss:0.27884814143180847
W:[[-0.9435631 ]
 [ 0.19123243]
 [ 0.7254163 ]], b:[0.45885307], loss:0.13230301439762115
W:[[-0.59015477]
 [ 0.01890054]
 [ 0.78149533]], b:[0.30215567], loss:0.07058969140052795
W:[[-0.35423252]
 [-0.08844119]
 [ 0.8041756 ]], b:[0.20206754], loss:0.04429158195853233
W:[[-0.19604999]
 [-0.15572144]
 [ 0.8088031 ]], b:[0.13853598], loss:0.03292479366064072
W:[[-0.08950195]
 [-0.19836576]
 [ 0.80423194]], b:[0.09863388], loss:0.027927277609705925
W:[[-0.01741161]
 [-0.22590864]
 [ 0.7957289 ]], b:[0.07397476], loss:0.025685466825962067
W:[[ 0.03161343]
 [-0.24417016]
 [ 0.7856612 ]], b:[0.05918323], loss:0.024654360488057137
W:[[ 0.06500661]
 [-0.25678405]
 [ 0.7763724 ]], b:[0.05054443], loss:0.024168530479073524
```

학습이 종료되어 w와 b가 최적화 되었다. 그리고 완성한 노드의 그림은 다음과 같다.

![노드그래프](./jpgfile/노드그래프.png)

이제 prediction 해보자. 예측값을 알기 위해서는 H노드를 사용해야 한다. H노드는 X노드로부터 오며 X노드는 placeholder이므로 값을 줘야한다.

```python
predict_data = np.array([[180.0, 10.0, 80.0]])
scaled_predict_data = scaler_x.transform(predict_data)

scaled_result = sess.run(H, feed_dict={X:scaled_predict_data})
result = scaler_y.inverse_transform(scaled_result.reshape(-1,1))

print(result)
------------------------------------------------------------------------------
[[41.981033]]
```

이제 사이킷런으로 구현한 값과 비교 해보자.

```python
# RAW DATA
df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Solar.R', 'Wind', 'Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')

# 2. 이상치(outlier) 처리, 지대점은 그냥 주자
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]

x_data = training_data.iloc[:,:-1].values
t_data = training_data['Ozone'].values

model = linear_model.LinearRegression()
model.fit(x_data, t_data)
sklearn_result = model.predict([[180.0, 10.0, 80.0]])
print(sklearn_result)
------------------------------------------------------------------------------
[41.59545428]
```

[[41.981033]] 와 [41.59545428]

이젠 거의 차이가 없게 되었다.




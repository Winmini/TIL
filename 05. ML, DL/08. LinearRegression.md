# 머신러닝, 딥러닝

## - Regression

어떤 data에 영향을 주는 조건들의 평균적인 영향력을 이용해서 그 data에 대한 조건부 평균을 구하는 방법을 Regression이라 한다.

예를 들어 아파트가격에 대해 영향을 주는 지역, 층수, 학군등의 영향력을 이용해 가격에 대해 구하는 방법이라고 생각하면 된다. 단적으로 말하면 회귀분석은 <u>평균</u>을 구하는 것이다.

회귀는 많은 회귀가 있지만 통계쪽에서 말하는 진짜 회귀랑 머신러닝에서 회귀는 살짝 다르다 여기서말하는 회귀란 클래식한 선형회귀이다.



간단한 예를 들어보자.

어떤 사람이 현재 우리나라 아파트의 시세가 얼마인지 조사한다고 가정해보자.

어떻게 해야 우리나라 아파트 시세의 **대표값**을 구할 수 있을까?

우리가 대표값을 주로 쓰는 것에는 평균값(mean), 중앙값(median),  최빈값(mode)이 있고.

평균에도 산술평균, 기하평균, 가중평균이 있다.

이런식으로 어쨋든 구했다고 가정해보자. 그러면 나한테 이 데이터가 쓸모가 있을까? 없다. 그럼 언제 쓸모가 있을까? 년도별 평균값, 즉 추이를 살펴볼 때에는 의미가 있다.

일반적인 용도로는 쓸모가 없는 이유는 뭘까?

아파트 가격에 영향을 주는 다양한 요인들이 있고, 그 요인에 따라 가격이 너무 많이 달라진다. 조금더 유용한 정보가 되려면, 다양한 조건에 따로 집계해야한다.

면적, 지역, 학군, 역세권, 년식, 아파트 방향.. 등에 따라 집계하는 것이 좋을 것이다. 이렇게 표를 만들면 유용한 정보를 제공하는 표를 만들 수 는 있겠지만 활용도가 떨어진다. 그래서 각 조건에 따라 가격이 평균적으로 어떻게 변하니? 를 구해보는 것이다.

가격 = 350만원 x 면적 + 6500만원

가격 = 2000만원 x 층수 + 2500만원

...

이런식으로 다른 조건은 무시한 채 이런식으로 식을 개별적으로 다 구해서 이 수식을 합친다.

어떻게 합치는 지는 생략하고, 결론적으로

평균가격 = (100x면적) + (200x층수) + (5000 x 역세권여부) + ...

등으로 구해질 수 있다. 이런 방식으로 평균가격을 구하는 방법을 회귀방법이라 한다.

이 수삭을 회귀모델이라 한다.

회귀모델을 쓰려면 원래 평균치를 가져다 쓸 때 평균이 정규분포의 평균이여야 한다. 하지만 실제로 해보면 정규분포가 아니더라도 회귀모델이 잘 들어 맞기는 한다.

회귀모델의 일반식은 다음과 같다.

![회귀모델](./jpgfile/일반식.png)



그러면 왜 Regression이라 부를까? - 옛날 이야기..

찰스 다윈(종의 기원, 진화론)의 사촌인 프랜시스 골턴이 종의 기원을 읽고 감명을 받는다. 그리고 인간개선을 목적으로  선택적 출산을 하는 '우생학'이라는 용어를 창안한다. 환경보다는 유전적인 요인이 더 큰 영향을 준다라는 것을 입증하기 위해 여러가지를 조사한다. 그 중 하나가 키였다. 근데 이상한게 키가 큰 사람의 자녀가 키가 크긴하지만 아버지만큼 크진 않았고, 키가 작은사람의 자녀는 키가 작지만 아버지만큼 작지는 않았다. 자식들은 다시 전체평균에 맞춰지고 있는 것이다. 그래서 이 것에 대해 Regression toward Mean (평균으로의 회귀)라고 말하였다.



---

### Simple Regression Problem

위에 회귀모델의 일반식에서 <u>특정변수가 1개</u>일 때  y = ax + b 와같이 표현할 수 있다.  이를 단순 선형회귀라고 하며, 이에 대해 한번 확인해보자. 머신러닝에서는 이 직선을 표현할 때에 y = wx + b로 표현한다. w는 weight의 약자로 가중치라고 하며, b는 bias, 바이어스라고 한다.

여러 직선의 기울기와 절편값을 랜덤하게 부여하였는데 이중 가장 대표하는 '것'같은 직선에 대해 생각해보자.

처음에는 기울기와 값을 랜덤으로 부여하고, 이 부여된 값을 조금씩 수정해가면서 제일 잘 표현하는 직선을 찾아가는 것이다. 이 찾아가는 과정 자체를 running(학습)이라고 한다. 



![에러그래프](./jpgfile/error그래프.png)

점은 실제 값들이고 우리가 예측한 값은 직선의 데이터에 해당한다고 예측했다고 해보자. 그렇다면 그에 대한 오차 (error)는 빨간 값의 크기와 같다. 당연히 이 오차는 작은게 좋다. 만약 오차값이 크다면 직선이 데이터를 잘 표현하지 못하고 있다는 얘기가 된다. 참고로 error를 그냥 더하면 +값과 -값의 error가 있을 테니 의도치않게 0에 가까워질 수 있다. 따라서 그냥 더하지 않고 절댓값을 취하게 되고, 제곱까지 해서 더하면 좋은 것은 더 좋아지고 안좋은 값은 더 안좋아지는 결과가 된다.

**Loss function(손실함수)**

![손실함수](./jpgfile/손실함수.png)

Training data set의 정답과 모델의 예측값을 더해서 수식으로 표현한 식, 오차의 제곱의 평균(MSE)을 이용하여 구한다. 

위에서 t는 label이고, x는 입력값이다. 둘 다 이미 결정되어있는 값이다.  따라서 loss function은 W와 b의 함수인 E(W,b)이다.

그렇다면 이 손실함수 E(w,b)를 최소로 만들어야 하며, 이 최소로 만드는 알고리즘에는 여러개가 있지만 먼저 Gradient Descent Algorithm(경사 하강법)을 이용할 것이다. 그리고, 이 손실함수는 자세히보면 2차함수이다. 한번 코드로 직접 확인해보면

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(1,101)
t = np.arange(1,101)

w = np.arange(-10, 30)

loss = []

for tmp in w:
    loss.append(np.power((t-tmp*x),2).mean())
plt.plot(w,loss)
plt.show()
```

![함수](./jpgfile/함수그래프.png)

이차함수임을 확인할 수 있다. loss가 최소가 되는 w와 b값을 찾는게 우리의 목적이다. 



찾는 방법에 대해 얘기해보자. 임의의 가중치 w를 선택한다. 그리고 그 지점에서의 손실함수의 점선의 기울기를 구한다. 그리고 목표지점으로 이동하기 위해 기울기가 가장 가파르게 되는 부분의 <u>반대</u>방향으로 w를 구한다. 

![수식](./jpgfile/수식.png)

그 방법으로 기울기와 learning rate를 곱해서 이동한다. 이 rate값을 곱하지 않으면 너무 많이 이동해버려서 가장 기울기가 낮은 부분을 지나쳐버릴 수도 있기 때문이다. 이 rate는 보통 0.001로 시작을 한다. 이렇게 이동하는 작업을 반복하다보면 기울기가 0인 지점에 도달했을 때 더 이상 이동을 하지 않게 된다.

여기까지의 설명부분을 jupyter notebook을 통해 한번 코드로 실행시켜보자.

```python
import numpy as np

# 1. Traning Data Set 준비
x_data = np.array([1,2,3,4,5]).reshape(5,1)
t_data = np.array([3,5,7,9,11]).reshape(5,1)

# 2. Weight & bias를 정의
w = np.random.rand(1,1)
b = np.random.rand(1)

# 3. loss function을 구현
def loss_func(input_value):
    # input_value안에 w와 b가 들어있어야 함.
    w = input_value[0].reshape(1,1)
    b = input_value[1]
    
    y = np.dot(x_data,w) + b
    
    return np.mean(np.power(t_data-y,2))


# 4. 미분을 수행할 함수, 저번에 썼던 함수 그대로 가져옴
def numerical_derivative(f, x):
    # f: 미분을 하려고하는 함수
    # x: 모든 독립변수의 값을 포함하고 있는 ndarray
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        idx = it.multi_index
        tmp = x[idx]
        x[idx] = tmp + delta_x
        fx_plus_deltax = f(x)
        
        x[idx] = tmp - delta_x
        fx_minus_deltax = f(x)
        
        derivative_x[idx] = (fx_plus_deltax - fx_minus_deltax) / (2 * delta_x)
        x[idx] = tmp
        
        it.iternext()
        
    return derivative_x

# 5 Learning rate 정의
learning_rate = 1e-4

# 6. 반복 학습을 진행해보자.
for step in range(100000):
    # 최적의 loss값인지 판단하기 힘드므로 반복을 많이 해본다.
    input_param = np.concatenate((w.ravel(), b.ravel()), axis=0)
    tmp = numerical_derivative(loss_func, input_param)*learning_rate
    
    w = w - tmp[0].reshape(1,1)
    b = b - tmp[1]
    
    if step%10000 == 0:
        print('W: {}, b: {}'.format(w,b))

------------------------------------------------------------------------------
W: [[0.84281246]], b: [0.11546265]
W: [[2.10346813]], b: [0.6264472]
W: [[2.07378547]], b: [0.73361106]
W: [[2.05261808]], b: [0.81003203]
W: [[2.03752314]], b: [0.86452955]
W: [[2.0267586]], b: [0.90339296]
W: [[2.01908216]], b: [0.93110733]
W: [[2.01360792]], b: [0.95087108]
W: [[2.00970412]], b: [0.96496506]
W: [[2.00692022]], b: [0.97501579]
```

이렇게 w값과 b값을 구했다. 기존 값을 예측하면 다음과 같고, 새로운 값을 이 모델에 넣으면

```python
h = np.dot(x_data,w) + b
print(h)
------------------------------------------------------------------------------
[[ 2.98711773]
 [ 4.99205287]
 [ 6.996988  ]
 [ 9.00192314]
 [11.00685827]]
```

다음과 같이 구할 수 있다.

```python
print(np.dot([[7]],w)+b)
------------------------------------------------------------------------------
[[15.01672854]]
```

Loss function에 대해 잠깐 얘기해보고 가자.

loss function의 모양은 comvex funtion이 형태가 되어야 한다. (볼록함수) 왜냐하면 w값이 계속 조정될 텐데

![손실함수](./jpgfile/손실함수그래프.png)

만약 국지적인 local minima가 있다면 위 그림에서 보다시피 자주색에서 w값이 시작되었다면, 국지적 최소값에서 Gradient Decent가 0이 되어서 더 이상 이동하지 않을 것이다. 이동하지 않는다는 것은 학습되지 않는다는 것이고, 진짜로 최소의 loss function은 찾을 수 없게 된다.

loss 함수를 지정할 때 제곱해서 평균을 구하는 방식으로 했었다. 사실 이 이유가 여기 있는 것이다. 단순히 절대값으로 평균을 구하면 이런 comvex함수가 나오지 않게 된다. 
# 머신러닝, 딥러닝

## - Normalization

데이터가 가진 scale이 심하게 차이가 나는 경우 학습이 잘 이루어지지 않는다. 따라서 scale을 맞춰주는 작업이 필요한데, 이를 Normalization이라 한다.



예를 들어 아파트의 가격을 계산할 때 방의 개수와 연식으로 집값을 계산한다고 해보자. 근데 방의개수가 1개인것과 4개인것의 집값은 엄청나게 차이나지만, 연식이 1년과 4년차이인것의 집값은 엄청나게 차이나지는 않는다. 같은 숫자의 1~4지만 끼치는 영향이 다르다. 이 끼치는 영향을 같은 비율로 조절하는 과정이 정규화이다.



많이 쓰는 2개의 정규화를 소개하겠다.

- Min Max Normalization: 가장 일반화된 Normalization방식
- z-score Normalization (= standardization)

---

### Min-Max Normalization

모든 feature들에 대해서 최소값 0 최대값 1사이의 값으로 scaling하는 방식.

Xscaled = (X - Xmin) / (Xmax - Xmin)

이와 같은 과정을 거쳐서 만든다. 만약 온도 데이터가 10, 30, 50, 60, 70 있다고 해보면, scale된 data는 0, 0.33, 0.67, 0.83, 1 로 스케일링 된다.

아주 간편한 방법이다. 하지만 문제가 조금 있다. 이상치가 존재하면 정규화에서 문제가 된다. 이상치에 굉장히 민감하게 반응하기때문에, 반드시 이상치를 처리하고나서 거쳐야 하는 작업이다.

---

### Z-Score Normalization

식만 조금 다르다

Xscaled = (X - Xmean) / Xstandard

로 스케일링 한다. min-max는 무조건 0~1이였지만 이 값은 그렇지 않으며, -값도 가질 수 있어서 동일한 척도를 가지지 않게 된다. 그럼 왜 쓸까? 이상치에 관대하다.

---



### 실습

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
from scipy import stats
from sklearn.preprocessing import MinMaxScaler

fig = plt.figure()
fig_python = fig.add_subplot(1,2,1)
fig_python.set_title('Using Python')
fig_sklearn = fig.add_subplot(1,2,2)
fig_sklearn.set_title('Using Sklearn')

def numerical_derivative(f, x):
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        idx = it.multi_index
        tmp = x[idx]
        x[idx] = tmp + delta_x
        fx_plus_deltax = f(x)
        
        x[idx] = tmp - delta_x
        fx_minus_deltax = f(x)
        
        derivative_x[idx] = (fx_plus_deltax - fx_minus_deltax) / (2 * delta_x)
        x[idx] = tmp
        
        it.iternext()
        
    return derivative_x

# Raw Data Loading
df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')
training_data.head()

# 2. 이상치(지대점) 처리
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Temp'])) < zscore_threshold]

# 3. 이상치(outlier) 처리
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]
display(result_data)
# 이상치에서 주의해야할점이 있다. 지대점과 outlier를 제거할 때 순서대로 제거한다하면
# 남은 데이터에서 다시 계산할 것이므로, 이상치 기준점이 바뀔 수 있다.


# 4. 정규화(Normalization)
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
# scaling 하는 객체 2개, 하나는 입력모두를, 하나는 출력을

scaler_x.fit(training_data['Temp'].values.reshape(-1,1))
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))   
# scale하여 저장

training_data['Temp'] = scaler_x.transform(training_data['Temp'].values.reshape(-1,1))
training_data['Ozone'] = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))

# Training Data Set
x_data = training_data['Temp'].values.reshape(-1,1)
t_data = training_data['Ozone'].values.reshape(-1,1)

# Weight & bias를 정의
W = np.random.rand(1,1)
b = np.random.rand(1)

def predict(x):
    
    return np.dot(x,W) + b    # y = Wx + b


def loss_func(input_obj):
    
    input_w = input_obj[0].reshape(-1,1)
    input_b = input_obj[1]
    y = np.dot(x_data,input_w) + input_b 
    return np.mean(np.power((t_data-y),2))

learning_rate = 1e-4

# 반복학습을 진행
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
    result_derivative = learning_rate * numerical_derivative(loss_func,input_param)
    
    W = W - result_derivative[0].reshape(-1,1)
    b = b - result_derivative[1]
    
    if step % 30000 == 0:
        print('loss : {}'.format(loss_func(input_param)))
------------------------------------------------------------------------------
loss : 0.08728742675922872
loss : 0.031490788105791236
loss : 0.03094710776161987
loss : 0.03065738461130933
loss : 0.03050298963503639
loss : 0.030420711747773912
loss : 0.030376865432979253
loss : 0.030353499503383393
loss : 0.030341047677763935
loss : 0.03033441203501658
```

로스값이 예전에 600이 나왔는데 0.03으로 줄었다. 학습을 더해도 거의 변하지 않으므로 학습량을 늘릴 필요는 없을 것 같고, 추가로 learning rate정도는 조절해볼 수 있을 것이다.

학습이 끝났으니 Prediction을 해보자.

```python
print(predict([[62]]))    # [[48.74690939]]
# 값이 많이 이상하다. 왜냐하면 scale해서 넣어야 하기 때문이다.
scaled_predict_data = scaler_x.transform([[62]])
print(scaled_predict_data)
scaled_result = predict(scaled_predict_data)    # [[0.03030303]]
# scale하고 결과를 그냥 볼 수 없다.

# 원래값으로 복구해야한다.
print(scaler_y.inverse_transform(scaled_result))   # [[3.35013811]]
------------------------------------------------------------------------------
[[48.74690939]]
[[0.03030303]]
[[3.35013811]]
```

이제 sklearn으로 구현한것과 비교해보자. 사이킷런을 사용하기 위해서 이상치값은 알아서 처리해주겠지만, 여기서는 같은데이터로 학습한 것을 비교하기 위해 데이터는 같은 데이터를 사용하게 하자.

```python
df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')
training_data.head()

# 2. 이상치(지대점) 처리
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Temp'])) < zscore_threshold]

# 3. 이상치(outlier) 처리
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]
display(training_data)  

# 여기까진 동일한 과정

model = linear_model.LinearRegression()

model.fit(training_data['Temp'].values.reshape(-1,1), 
          training_data['Ozone'].values.reshape(-1,1))

result = model.predict([[62]])

print(result)
------------------------------------------------------------------------------
[[1.75864872]]
```

사이킷런은 [[1.75864872]], 직접한 파이썬은 [[3.35013811]]으로 꽤 많이 근접해졌다. 물론 싸이킷런은 더 복잡한 알고리즘이기 때문에 조금 차이가 나지만 초반에 차이가 있었던 부분은 많이 메꿔졌다. 그래프로 차이를 비교해보자.

```python
fig = plt.figure()

fig_python = fig.add_subplot(1,2,1)
fig_sklearn = fig.add_subplot(1,2,2)

fig_python.set_title('Using Python')
fig_sklearn.set_title('Using sklearn')
#################################

fig_python.scatter(x_data,t_data)
fig_python.plot(x_data, x_data*W.ravel() + b, color='r')

fig_sklearn.scatter(training_data['Temp'].values, 
                    training_data['Ozone'].values)
fig_sklearn.plot(training_data['Temp'].values, 
                 training_data['Temp'].values*model.coef_.ravel() + model.intercept_, color='g')

fig.tight_layout()
plt.show()
```

![비교](./jpgfile/비교그래프2.png)

이제 거의 비슷해졌음을 확인할 수 있다.

여기까지가 머신러닝중 기본적이며 고전적인 방법중 단일데이터에 대해한 부분이다. 여기까지도 상당히 난이도가 있었음을 알 수 있다. 머신러닝을 모르는 사람도 이용할 수 있도록 하는 것이 사이킷런이므로 원래는 결치처리정도만 하고 간편하게 사용할 수 있다.

```python
df = pd.read_csf('./data/ozone/ozone.csv', sep=',')

training_data_set = df[['Temp', 'Ozone']]
trainig_data = training_data_set.dropna(how='any')

model = linear_model.LinearRegression()

model.fit(training_data['Temp'].values.reshape(-1,1),
         training_data['Ozone'].values.reshape(-1,1))
```

이 코드가 사실은 전부이다. 하지만 학습하는 과정에서 어떤작업이 있어야함을 알아야하고, 전문적인 데이터를 다루기 위해서는 직접 데이터 전처리작업을 해줘야 하므로 꼭해야 한다.



---

### 실습2

위에서는 독립변수가 1개인 simple 데이터에 대해서 해보았다면 나머지 독립변수까지 포함한 다중선형회귀(multiple Linear Regression)에 대해서도 해보자.

![회귀모델](./jpgfile/일반식.png)

이 식을 다시 한번 보면 x의 개수에 다라 b의 개수 즉, 구해지는 가중치값 w가 늘어남을 알 수 있다. 우리가 독립변수 3개를 쓰면 가중치도 3개를 구해야 한다.  행렬곱 연산으로 표현하면

y = [x1 + x2+ x3]*w + b 이다.



```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
from scipy import stats
from sklearn.preprocessing import MinMaxScaler

### 수치미분함수 ###

def numerical_derivative(f, x):
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        
        tmp = x[idx]    
               
        x[idx] = tmp + delta_x   
        fx_plus_deltax = f(x)
        
        x[idx] = tmp - delta_x   
        fx_minus_deltax = f(x)
        
        derivative_x[idx] = (fx_plus_deltax - fx_minus_deltax) / (2 * delta_x)
        
        x[idx] = tmp     
        
        it.iternext()

    return derivative_x


df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Solar.R', 'Wind', 'Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')

# 2. 이상치(outlier) 처리, 지대점은 그냥 주자
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]

# 3. 정규화 - Min-Max Normalization
scaler_x = MinMaxScaler()
# 입력값 전체를 담당할 것이므로 여전히 1개만 만든다.
scaler_y = MinMaxScaler()

scaler_x.fit(training_data.iloc[:,:-1].values)  
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))   

training_data.iloc[:,:-1] = scaler_x.transform(training_data.iloc[:,:-1].values)
training_data['Ozone'] = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))

# Training Data Set
x_data = training_data.iloc[:,:-1].values
t_data = training_data['Ozone'].values.reshape(-1,1)

# Weight & bias를 정의
W = np.random.rand(3,1)
b = np.random.rand(1)

def predict(x):
    return np.dot(x,W) + b    # y = Wx + b


def loss_func(input_obj):
# [W1의 값, W2의 값, W3의 값, b의 값]  1차원 ndarray
    input_w = input_obj[:-1].reshape(-1,1)
# 행렬곱연산을 수행해야 하니까 2차원으로 표현
    input_b = input_obj[-1:]
    
    y = np.dot(x_data,input_w) + input_b
# 입력값에 대해 현재 W와 b를 이용한 예측치 계산
    
    return np.mean(np.power((t_data-y),2))

learning_rate = 1e-4

# 반복학습을 진행
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W1의 값, W2의 값, W3의 값, b의 값]
    result_derivative = learning_rate * numerical_derivative(loss_func,input_param)
    
    W = W - result_derivative[:-1].reshape(-1,1)
    b = b - result_derivative[-1:]
    
    if step % 30000 == 0:
        print('loss : {}'.format(loss_func(input_param)))
------------------------------------------------------------------------------
loss : 0.3056263308610974
loss : 0.04381892993552699
loss : 0.03443570816746064
loss : 0.02948516730335741
loss : 0.026858769496214872
loss : 0.025453985945622135
loss : 0.024693525951524607
loss : 0.02427461904985882
loss : 0.024038124866645957
loss : 0.023900137177845233
```

0.02로 이전보단 더 좋게 나왔다. 사실 완전 잘나왔다 하기엔 아쉽긴하다. 그리고 우리의 모델이 제대로 되었는지 Regression은 사실 판단하기 쉽지 않다. 로지스틱모델은 그래도 판단하기 괜찮은데 우리는 잘 알 수 없어서 사이킷런과 비교하고 있지만 사이킷런이 없으면 잘 만들어졌는지 잘 모를 수 있다.

마지막으로 예측 비교하는것 까지 다시 해보자.

```python
scaled_predict_data = scaler_x.transform([[180, 10, 62]])
scaled_result = predict(scaled_predict_data)
print(scaler_y.inverse_transform(scaled_result))
------------------------------------------------------------------------------
[[9.8916523]]
```

사이킷런

```python
df = pd.read_csv('./data/ozone/ozone.csv', sep=',')
training_data_set = df[['Solar.R', 'Wind', 'Temp', 'Ozone']]

# 1. 결치값 처리
training_data = training_data_set.dropna(how='any')

# 2. 이상치(outlier) 처리, 지대점은 그냥 주자
zscore_threshold = 1.8
training_data = training_data[np.abs(stats.zscore(training_data['Ozone'])) < zscore_threshold]

model = linear_model.LinearRegression()

model.fit(training_data[['Solar.R', 'Wind' ,'Temp']].values,
         training_data['Ozone'].values.reshape(-1,1))
result = model.predict([[180, 10, 62]])
print(result)
------------------------------------------------------------------------------
[[11.0682182]]
```

얼추 비슷하게 나오는 것 같으므로 여기까지 한다.

그리고 살짝 문제가 되는 부분은 사이킷런은 엄청나게 빠르지만, 직접 구현한 부분은 엄청 느리다. 몇줄 되지도 않는데 벌써 허덕이고 있다. 데이터 100개에서도 이렇다. 나중에 머신러닝에선 데이터 만개~십만개씩 들어가는데 이 과정이 제대로 진행될리 없다.

물론 코드도 상당히 low레벨이다. 그래서 좋은 라이브러리(구글의 텐서플로)혹은 프레임워크들이 있으며, 이를 사용해볼 예정이다. ~~이제 더러운 미분코드같은건 사용 안한다.~~
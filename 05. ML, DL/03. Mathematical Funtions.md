# 머신러닝, 딥러닝

## - Mathematical Functions



### Ndarray

수학적인 함수들에 대해서도 알아보자.

다음은 좀 일반적인 내용이다.

```python
import numpy as np
arr = np.array([4, 6, 1, 3, 8, 8])

# 합
print(arr.sum())

# 평균
print(arr.mean())

# 편차(deviation): 확률변수 X와 평균의 차이
# 분산: 편차의 제곱의 평균
print(arr.var())

# 표준편차
print(arr.std())
------------------------------------------------------------------------------
30
5.0
6.666666666666667
2.581988897471611
```



**공분산(covariance)**

두 확률변수의 관계를 보여주는 값이다.

ex) y = 10x + 3 과 같은 관계를 가지면 x가 변하면 y가 변한다.

COV(X,Y) = sum ((Xi - Xm)*(Yi - Ym) / (n-1)

이 값은 X가 변할 때 Y가 변하는 정도를 나타내는 값.

```python
import numpy as np
arr1 = np.array([4, 6, 1, 3, 8, 8])
arr2 = np.array([2, 3, 1, -1, 9, 7])
# 2개가 있어야 COV를 구할 수 있다.
# 하나가 종속되어 있다고 가정.
# arr1이 4일때 arr2는 2, arr1이 6일때 arr2는 3 .. 등등..
print(np.cov(arr1,arr2))
------------------------------------------------------------------------------
[[ 8.   9.4]
 [ 9.4 14.3]]
```

의미를 따져보면,

양수값이 나왔다: 독립변수가 증가하면 종속변수는 증가함을 의미

음수값이 나왔다: 독립변수가 증가하면 종속변수는 감소함을 의미

나온 수치는 크게 의미가 없다. 사용하기에 제약이 있다. => 사용 빈도가 적다.



데이터를 통해실제 공분산을 구해서 알아보자.

KOSPI와 삼성전자에 대한 관계를 알아보자.

데이터가 있어야 하니 pandas의 datareadr를 이용해서 주가데이터를 가져올 것이다.

anaconda를 실행시켜 `pip install pandas_datareader`를 실행시켜 다운받자.

```python
import numpy as np
import pandas as pd
import pandas_datareader.data as pdr
from datetime import datetime

start = datetime(2018,1,1)
end = datetime(2018,12,31)

pdr.DataReader('^Ks11','yahoo',start,end)
# 우리나라 코스피에 대한 DATA를 가져온다. 
------------------------------------------------------------------------------
	High	Low	Open	Close	Volume	Adj Close
Date						
2018-01-03	2493.399902	2481.909912	2484.629883	2486.350098	331100.0	2486.350098
2018-01-04	2502.500000	2466.449951	2502.500000	2466.459961	333800.0	2466.459961
2018-01-05	2497.520020	2475.510010	2476.850098	2497.520020	308800.0	2497.520020
2018-01-08	2515.370117	2494.179932	2510.699951	2513.280029	311400.0	2513.280029
2018-01-09	2524.729980	2498.409912	2510.729980	2510.229980	374300.0	2510.229980
...	...	...	...	...	...	...
2018-12-21	2061.510010	2049.760010	2052.699951	2061.489990	311400.0	2061.489990
2018-12-24	2059.939941	2046.180054	2050.379883	2055.010010	285300.0	2055.010010
2018-12-26	2037.829956	2014.280029	2028.810059	2028.010010	321500.0	2028.010010
2018-12-27	2035.569946	2021.390015	2032.089966	2028.439941	398000.0	2028.439941
2018-12-28	2046.969971	2035.410034	2036.699951	2041.040039	352700.0	2041.040039
```

데이터를 가져오는데 성공했으면 삼성전자 주식도 가져오고 json으로 바꾸면서 진행하자

```python
df_kospi = pdr.DataReader('^Ks11','yahoo',start,end)
df_se = pdr.DataReader('005930.KS','yahoo',start,end)
df_kospi.to_json('./data/KOSPI.json')
df_se.to_json('./data/SE.json')

df_close_kospi = df_kospi['Close']
df_close_se = df_se['Close']
print(df_close_se)
# print해보면 알겠지만, series형태로 출력된다. 따라서 공분산에 이용해보자.

np.cov(df_close_kospi.values, df_close_se.values)
------------------------------------------------------------------------------
Date
2018-01-03    51620.0
2018-01-04    51080.0
2018-01-05    52120.0
2018-01-08    52020.0
2018-01-09    50400.0
               ...   
2018-12-21    38650.0
2018-12-24    38800.0
2018-12-26    38350.0
2018-12-27    38250.0
2018-12-28    38700.0
Name: Close, Length: 242, dtype: float64
            
array([[   24045.56247333,   489067.36939603],
       [  489067.36939603, 11918322.34148349]])
```

의미하는바:

- 24045: KOSPI의 공분산
- 11918322: SAMSUNG의 공분산
- 489067: KOSPI와 SAMSUNG의 공분산, 양수이므로 추세가 같다.



확인을 위해 서로 반대의 상관관계를 가지는 것도 한번 공분산을 구해보자.

방산관련주와, 남북협력관련주는 서로 반대로 움직일것을 예상해볼 수 있으니 이를 가져와보자.

```python
df_LIG = pdr.DataReader('079550.KS','yahoo',start,end)  # 방산주
df_BUSAN = pdr.DataReader('011390.KS','yahoo',start,end)# 협력주

print(np.cov(df_LIG['Close'].values, df_BUSAN['Close'].values))
------------------------------------------------------------------------------
[[ 6.24988174e+07 -3.81494283e+08]
 [-3.81494283e+08  4.64412566e+09]]
```

-값 임을 확인할 수 있다.

하지만 공분산은 수치에 의미를 부여하기가 참 애매하다. 

그래서 다른 값을 이용한다.



**상관관계(Correlation)**: 두 대상이 서로 연관성이 있다고 추측되는 관계

- 성적과 자존감
- 온라인 게임시간과 폭력성

**상관계수(Correlation Coefficient)**: -1 과1 사이의 실수값을 가진다. 0과 가까워질수록 연관성이 없는 것이며, 1이나 -1로 가까워질수록 연관성이 높다. 참고로 상관계수를 구하는 방법은 되게 여러가지인데 일반적으로 말하는 상관계수는 피어슨 상관계수이다.

상관관계를 이용할 때 주의할점이 하나 있다. 인과관계는 상관계수로 설명할 수 없다.

예를 들면 성적이 높은 사람이 자존감이 높다라고 해도 성적이 높기 때문에 자존감이 높아졌다. 라고 할 수 는 없는 것이다. 어떤 원인과 결과를 얘기할 수는 없다. 오류를 범할 여지가 많기 때문이다.

한번 위의 예시를 이용하여 상관계수 값을 구해보자.

cov대신에 corrcoef함수를 이용하면 구할 수 있다.

```python
print(np.corrcoef(df_LIG['Close'].values, df_BUSAN['Close'].values))
------------------------------------------------------------------------------
[[ 1.         -0.70810906]
 [-0.70810906  1.        ]]
```

서로 반대로 움직이므로 -값이고 0.7이라는 값은 꽤 반대로 움직인다 정도로 해석된다.



DataFrame을 이용하여 구하기

DataFrame과 np의 함수명이 살짝 다르니 주의하자.

```python
df = pd.DataFrame({
    'KOSPI': df_close_kospi.values,
    'SE': df_close_se.values
})
print(df)
df.corr()
------------------------------------------------------------------------------
           KOSPI       SE
0    2486.350098  51620.0
1    2466.459961  51080.0
2    2497.520020  52120.0
3    2513.280029  52020.0
4    2510.229980  50400.0
..           ...      ...
237  2061.489990  38650.0
238  2055.010010  38800.0
239  2028.010010  38350.0
240  2028.439941  38250.0
241  2041.040039  38700.0

[242 rows x 2 columns]

	KOSPI	SE
KOSPI	1.000000	0.913574
SE	0.913574	1.000000
```

0.9면 거의 붙어다니는 수준이다.

참고로 기준을 보통 0.4과 0.7을 기준으로 둔다.

0.7보다 크면 거의 붙어다니는 경향성, 0.4~0.7은 그래도 어느정도 붙어다닌다. 0.4미만은 좀 얘기하기 애매하다. 정도로 얘기한다.

---



### DataFrame

DataFrame의 함수에 대해서도 알아보자.

```python
import numpy as np
import pandas as pd
data = [[2,np.nan],[7,-3],[np.nan,np.nan],
       [1,-2]]
df = pd.DataFrame(data)
display(df)
# 그냥 list로만으로도 만들 수 있다.
------------------------------------------------------------------------------
	0	1
0	2.0	NaN
1	7.0	-3.0
2	NaN	NaN
3	1.0	-2.0
```

하지만 컬럼명을 명시해서 만들어주자.

```python
df = pd.DataFrame(data, columns=['one', 'two'], index=['a','b','c','d'])
display(df)
------------------------------------------------------------------------------
	one	two
a	2.0	NaN
b	7.0	-3.0
c	NaN	NaN
d	1.0	-2.0
```

DataFrame의 **Sum**

```python
df.sum()
# df.sum(axis=0, skipna=True)가 디폴트 표현이다.
# 따라서 NaN값은 무시하거나 0으로 처리한다.
df['two'].sum()
# 열하나 더하기
df.loc['b'].sum()
# 행하나 더하기
df.loc['a'].sum(skipna=False)
# 연산이 가능하나 만약 nan과 연산을 수행하면 무조건 결과는 nan이 나온다.
------------------------------------------------------------------------------
one    10.0
two    -5.0
dtype: float64
-5.0
4.0
nan
```

nan은 당연히 없는게 좋다.

**nan을 처리하는 방법**

- 데이터를 지우는 방법: 데이터량이 방대하고 상대적으로 매우 적은 nan이 있다면 그 데이터(record 통쨰로)를 지우는게 최적의 방법이다. 하지만 지금처럼 데이터에 비해 nan이 많다면 지우면 데이터 유실이 너무 치명적이라 좋지 않다.
- 데이터를 대체하는 방법: 결치값(nan)을 다른 값으로 대체할 수 있는데, 그 값도 여러가지가 있다. 대표적인 값으로 평균값을 많이 넣지만 그외에도 최대값 최소값을 넣기도 한다. 더 좋은 방법은 나머지데이터를 가지고 머신러닝프로그램을 돌려서 값을 도출하여 넣는 방법도 있다.

```python
one_mean = df['one'].mean()
two_mean = df['two'].mean()
print(one_mean)
print(two_mean)
df['one'].fillna(one_mean, inplace=True)
df['two'].fillna(two_mean, inplace=True)
# nan값을 채우며, 원본을 바꾼다.

------------------------------------------------------------------------------
3.3333333333333335
-2.5
3.3333333333333335
-2.5
        one  two
a  2.000000 -2.5
b  7.000000 -3.0
c  3.333333 -2.5
d  1.000000 -2.0
```



**DataFrame의 정렬**

정렬하기 위해 데이터를 먼저 섞어보자. 

```python
import numpy as np
import pandas as pd
np.random.seed(1)
df = pd.DataFrame(np.random.randint(0,10,(6,4)))
df.columns = ['A','B','C','D']
df.index = pd.date_range('20210801',periods=6)
# 날짜를 index로 잡아보자.
display(df)
# numpy에서 random하게 섞는 함수 shuffle(), np.random.shuffle()
# shuffle은 원본을 랜덤하게 섞는다.
np.random.shuffle(df.index)
# Index does not support mutable operations ERROR!
# columns이나 index는 값을 섞을 수 없다.
random_date = np.random.permutation(df.index)
# 원본을 그대로 두고 복사본을 만들어서 이것은 가능하다.
df.index = random_date
# 이렇게 바꾸면 된다.

# 근데 이렇게 바꾸면 이상하다. 왜냐하면 인덱스에 대한 정보가 바뀌었기 때문이다.
# index를 그냥 대체 해버렸기 때문이다. 우리는 데이터의 순서를 바꾸고 싶었는데,
# 데이터의 정보를 바꾼셈이 된다.

# 따라서 reindex를 사용해야 한다.
new_df = df.reindex(index=random_date, columns=['B','A','D','C'])
# 이렇게 하면 데이터가 따라다닌다. 컬럼도 변경할 수 있다.
display(new_df)
-------------------------------------------------------------------------------
            A  B  C  D
2021-08-01  5  8  9  5
2021-08-02  0  0  1  7
2021-08-03  6  9  2  4
2021-08-04  5  2  4  2
2021-08-05  4  7  7  9
2021-08-06  1  7  0  6

array(['2021-08-03T00:00:00.000000000', '2021-08-01T00:00:00.000000000',
       '2021-08-04T00:00:00.000000000', '2021-08-05T00:00:00.000000000',
       '2021-08-02T00:00:00.000000000', '2021-08-06T00:00:00.000000000'],
      dtype='datetime64[ns]')

            A  B  C  D
2021-08-03  5  8  9  5
2021-08-01  0  0  1  7
2021-08-04  6  9  2  4
2021-08-05  5  2  4  2
2021-08-02  4  7  7  9
2021-08-06  1  7  0  6

            B  A  D  C
2021-08-03  9  6  4  2
2021-08-01  8  5  5  9
2021-08-04  2  5  2  4
2021-08-05  7  4  9  7
2021-08-02  0  0  7  1
2021-08-06  7  1  6  0
```

데이터를 섞기에 성공했으니 이제 정렬해보자.

**정렬**

거의 계속 나온다. 정렬을 하는게 기본이라서 꼭 알고 가야한다.

```python
sort_df1 = new_df.sort_index(axis=0, ascending=True)
sort_df2 = new_df.sort_index(axis=1, ascending=True)
# ascending은 기본이 오름차순, True이다.
display(sort_df1)
display(sort_df2)
sort_df3 = new_df.sort_values(by='D')
# 특정 컬럼으로 정렬할 수 있다.
sort_df = new_df.sort_values(by=['A','D'])
display(sort_df)
# A부터 정렬하되, 값이 같으면 D의 오름차순으로 정렬
-------------------------------------------------------------------------------
			B	A	D	C
2021-08-01	8	5	5	9
2021-08-02	0	0	7	1
2021-08-03	9	6	4	2
2021-08-04	2	5	2	4
2021-08-05	7	4	9	7
2021-08-06	7	1	6	0

			A	B	C	D
2021-08-03	6	9	2	4
2021-08-01	5	8	9	5
2021-08-04	5	2	4	2
2021-08-05	4	7	7	9
2021-08-02	0	0	1	7
2021-08-06	1	7	0	6

			B	A	D	C
2021-08-02	0	0	7	1
2021-08-06	7	1	6	0
2021-08-05	7	4	9	7
2021-08-04	2	5	2	4
2021-08-01	8	5	5	9
2021-08-03	9	6	4	2
```

새로운 column을 추가하여 다른 함수에 대해서도 알아보자.

```python
df['E'] = ['CC','BB','CC','CC','AA','CC']
display(df)
df['E'].unique()
# 중복을 배제하여 값을 찾아오며, 정렬을 해서 가져오지는 않는다.
df['E'].value_counts()
# 값들이 몇개있는지 출력해주며, Series로 떨어진다.
df['E'].isin(['AA','BB'])
# AA와 BB만 있으면 뽑아내라!
# 이렇게 Boolean mask 생성이 가능하다
------------------------------------------------------------------------------
			A	B	C	D	E
2021-08-01	5	8	9	5	CC
2021-08-02	0	0	1	7	BB
2021-08-03	6	9	2	4	CC
2021-08-04	5	2	4	2	CC
2021-08-05	4	7	7	9	AA
2021-08-06	1	7	0	6	CC

array(['CC', 'BB', 'AA'], dtype=object)

CC    4
BB    1
AA    1
Name: E, dtype: int64
        
2021-08-01    False
2021-08-02     True
2021-08-03    False
2021-08-04    False
2021-08-05     True
2021-08-06    False
Freq: D, Name: E, dtype: bool
```


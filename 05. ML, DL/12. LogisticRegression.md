# 머신러닝, 딥러닝

## - Logistic Regression



machine learning model이 어떤 값을 예측하는가에 따라 2가지로 나뉜다고 했다.

- Regression(회귀): Continuous value
- Classification(분류): Discrete value

![분류](./jpgfile/분류.png)

어떤 점수가 있을 때 이에 해당하는 합격 불합격이 있다고 해보자. 합격과 불합격은 명백한 분류이다. 그 중간이 없다. 

Linear Regression을 확장해서 Classification model을 만들어보자. 이전에 했던 것은 Linear Regression이였다. 이것가지고는 분류문제를 해결할 수 없다. 이 분류문제를 해결하기 위해서 만들어진 것이 Logistic Regression이다.



초창기(1960~1970년) 인공지능 알고리즘중 가장 유명한게 Perceptron이 있다. 이를 발전시킨게 Logistic Regression이다. 자세한 설명은 생략하고 개념도를 보자

![개념도](./jpgfile/개념도.PNG)

y의 결과 값은 discrete value이다. 연속값이 들어가서 최종적으로 이산값을 뽑아내는게 이 모델이다.

어디 부분에 응용할 수 있을까?

- Email spam 판별
- 주식, 채권가격이 상승할지 하락일지
- MRI, CT의료용 사진을 이용한 병의 여부
- 신용카드 사용시 도난카드인지 판단
- 불량품 판단

이런 문제가 많을까? 굉장히 많다. 해결해야하는 대부분의 문제가 분류라고 봐도 무방하다. 정말 압도적으로 많다.

먼저 Activation Function으로 시그모이드부터 사용해보자. 시그모이드를 사용할 경우 그 결과로 나오는 값이 0과 1사이의 실수로 떨어진다.

그다음 임계함수는 0.5보다 크거나 같으면, 그렇지 않으면 0으로 만든다. 그래서 둘 중 하나인 것이다. 이것을 확장하여 여러개중 하나로 바꿀 수 있다.

---

### 실습

mglearn이라는 module을 설치하여 그 안에서 적절한 data set을 가져오자.

anaconda로 data_env로 가서 `pip install mglearn`으로 설치해주자.

```python
import numpy as np
from sklearn import linear_model
import mglearn
import matplotlib.pyplot as plt
# import warnings
# warnings.filterwarnings(action='ignore')
# 이 2줄로 각종 워닝을 무시할 수 있다.

# Training Data Set
x_data, t_data = mglearn.datasets.make_forge()
print(x_data)
# 2차원 ndarray, 2개의 column
# 0번째 column이 x좌표, 1번째 column이 y좌표

print(t_data)
# x_data로 표현되는 좌표평면의 점들이 어떤 값을 가지는지 알려주는 데이터

# plt로 그리기가 힘들다. 그림은 mglearn을 통해 그려서 보자.
mglearn.discrete_scatter(x_data[:,0],x_data[:,1], t_data)
------------------------------------------------------------------------------
[[ 9.96346605  4.59676542]
 [11.0329545  -0.16816717]
 [11.54155807  5.21116083]
 [ 8.69289001  1.54322016]
 [ 8.1062269   4.28695977]
...
 [11.563957    1.3389402 ]]
[1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0]
```

![mglearn그래프](./jpgfile/mglearn그래프.png)

위와 같이 그래프를 볼 수 있다. 1은 세모, 0은  동그라미에 해당한다. 위코드에 이어서 학습도 해보자.

```python
model = linear_model.LinearRegression()
model.fit(x_data[:,0].reshape(-1,1), x_data[:,1].reshape(-1,1))
# x좌표값을 x_data로 y좌표값을 t_data로 하여 학습한다.
# 그래서 이데이터를 가장 잘표현하는 직선으로 표시해보자.
print(model.coef_, model.intercept_)
# 기울기와 절편데이터를 가지고 직선을 그릴 수 있다.

plt.plot(x_data[:,0], x_data[:,0]*model.coef_.ravel() + model.intercept_, color='r')
plt.show()
```



![구분](./jpgfile/구분그래프.png)

Linear Regression으로 구분하는 데이터를 만들 수가 있다. 이러한 직선을 통해 위 아래로 나누고 위는 1, 아래는 0으로 값을 매기는 것으로 분류를 할 수 있다. 선과 가까울수록 0.5 선과 멀어질수록 0.7~9와 같이 확률도 제공한다.



분류문제를 Linear Regression으로 해결할 수 있을까? 간단한 실습을 확인해보자

```python
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

# Training Data Set
x_data = np.array([1,2,5,8,10,30]).reshape(-1,1)
# 공부시간에 따른
t_data = np.array([0,0,0,1,1,1]).reshape(-1,1)
# 시험 합격 여부

# Model 생성
model = linear_model.LinearRegression()

# Model 학습
model.fit(x_data, t_data)

print('W: {}, b: {}'.format(model.coef_, model.intercept_))
print(model.predict([[8]]))

plt.scatter(x_data.ravel(), t_data.ravel())
plt.plot(x_data.ravel(),
         x_data.ravel() * model.coef_.ravel() + model.intercept_,
         color = 'r')
------------------------------------------------------------------------------
W: [[0.03500583]], b: [0.17327888]
[[0.45332555]]
```

![그래프](./jpgfile/이상데이터.png)

데이터가 좀 튀는 데이터가 있는 순간, 안맞아 떨어진다. 8시간도 합격으로 데이터를 입력했지만,  확인해보면 0.45로 불합격쪽에 속한다. 심지어 30은 이상치도 아니다. 이상치란 존재하기 힘든 값이지만 실제로 30시간을 공부한사람이 있을 수 있으므로 제대로 입력한 값이 맞다.

그래서 LinearRegression으로는 해결하기가 힘들다.

따라서 Logistic으로 해야하며, 시그모이드부터 해보자. 그렇다면 모델은 어떻게 될까?

로지스틱 회귀모델은 linear의 응용버전으로 그  linear의 결과모델에 시그모이드함수만 취하면 된다. 시그모이드 함수가 f(x)라 하면, y = f(wx+b)가 되는 것이다.

파이썬구현으로 한번 확인해보자

```python
# 공부시간에 따른 합격여부

# python 구현

import numpy as np
import tensorflow as tf
from sklearn import linear_model

## 수치미분함수
def numerical_derivative(f, x):
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        
        tmp = x[idx]    
                
        x[idx] = tmp + delta_x   
        fx_plus_deltax = f(x)
        
        x[idx] = tmp - delta_x   
        fx_minus_deltax = f(x)
        
        derivative_x[idx] = (fx_plus_deltax - fx_minus_deltax) / (2 * delta_x)
        
        x[idx] = tmp     
        
        it.iternext()

    return derivative_x

# Training Data Set
x_data = np.arange(2,21,2).reshape(-1,1)   # 공부시간
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)
# 12시간은 불합격, 14시간은 합격, 13시간??

# Weight & bias
W = np.random.rand(1,1)
b = np.random.rand(1)

# predict
def logistic_predict(x):
    
    z = np.dot(x,W) + b    # linear regression
    y = 1 / ( 1 + np.exp(-1 * z) )  # sigmoid 적용, y가 우리의 model
    
    result = 0
    
    if y < 0.5:
        result = 0
    else:
        result = 1
    
    return result, y

# loss function
def loss_func(input_value):   # [W의값 ,b의값]  1차원 ndarray로 입력인자를 사용
    
    input_w = input_value[0].reshape(-1,1)
    input_b = input_value[1]
    
    z = np.dot(x_data,input_w) + input_b
    y = 1 / ( 1 + np.exp(-1 * z) )
    
    delta = 1e-7
    
    return -np.sum(t_data*np.log(y+delta) + (1-t_data)*np.log(1-y+delta))

# learning rate 설정
learning_rate = 1e-4

# 반복학습
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
    # [W의값 ,b의값]
    result_derivative = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - result_derivative[0].reshape(-1,1)
    b = b - result_derivative[1]
    
    if step % 30000 == 0:
        print('loss : {}'.format(loss_func(input_param)))
------------------------------------------------------------------------------
loss : 34.552432864100254
loss : 2.979362410904073
loss : 2.1525428201108205
loss : 1.7919188055660753
loss : 1.5786253235347296
loss : 1.4331004732445696
loss : 1.3252492234724311
loss : 1.2408949371929379
loss : 1.1723763693612324
loss : 1.1151425394009837
```

예측도 해보자

```python
# prediction
result = logistic_predict([[13]])
print(result)
------------------------------------------------------------------------------
(1, array([[0.54447942]]))
```

붙었고, 확률까지 본다면 0.544의 확률로 붙는다.



sklearn으로 구현한 것과 비교해보자.

```python
model = linear_model.LogisticRegression()
model.fit(x_data, t_data.ravel())
# t데이터는 1차원으로 입력한다
result = model.predict([[13]])
proba_result = model.predict_proba([[13]])
print(result)
print(proba_result)
------------------------------------------------------------------------------
[0]
[[0.50009391 0.49990609]]
```

0.50009391로 떨어지고 0.4999069로 붙어서 결과는 떨어진다고 예측하고 있다.



tensorflow도 해보자.

```python
import numpy as np
import tensorflow as tf
x_data = np.arange(2,21,2).reshape(-1,1)   # 공부시간
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

X = tf.placeholder(shape=[None,1], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

W = tf.Variable(tf.random.normal([1,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

# hypothesis
logit = tf.matmul(X,W) + b # linear
H = tf.sigmoid(logit) # logistic

# loss function
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                             labels=T))
# 예측값과 진짜값의 차이가 loss라고 생각하면 인자값이 이해가 된다.

# train node
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 반복학습
for step in range(300000):
    tmp,loss_val = sess.run([train,loss], feed_dict={X:x_data, T:t_data})
    if step % 30000 == 0:
        print('loss값: {}'.format(loss_val))
------------------------------------------------------------------------------
loss값: 1.3627349138259888
loss값: 0.4498034417629242
loss값: 0.4094923138618469
loss값: 0.3776502013206482
loss값: 0.35191377997398376
loss값: 0.3306705355644226
loss값: 0.3128618001937866
loss값: 0.2976542115211487
loss값: 0.28453555703163147
loss값: 0.27308279275894165
```

예측까지 해보자

```python
result = sess.run(H, feed_dict={x:[[13]]})
print(result) # 얘는합격
------------------------------------------------------------------------------
[[0.5767388]]
```

결과가 다 조금 씩 다르다.

어느게 맞을까? 라고 하면 사실 크게 의미 없다. 지금 데이터가 10개밖에 없어서 아무리 좋은 사이킷런이라도 예측하는게 별 의미 없는 것이다. 다만 어떻게 구현할 수 있는지와 데이터가 많아지면 더학습을 잘할테니 그렇게알고 넘어가자.



---

### 실습

미국대학원의 합격률데이터 admission이다. 이 파일로 한번 실습해보자.

prediction은 [600 3.8 1]인사람이 합격할까?



1. python구현

```python

```

2. sklearn 구현

```python

```

2. tensorflow구현

```python

```



대충 이정도나오면 맞다

---



근데 평상시에 내가 이걸 검증할 수 있는 방법이 있을까? 성능이 어느정도일까? 이런 판단을 어떻게해야할까 다행히 로지스틱회귀는 성능을 평가하는 지표(Metric)가 있다.

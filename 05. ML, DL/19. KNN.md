# 머신러닝, 딥러닝

## - KNN

머신러닝의 또 다른 기법으로 K-Nearest Neighbor(K-최근접이웃)의 약자이다. 상당히 간단한 머신러닝 알고리즘이다. 데이터가 흩뿌려져있을 때 알고싶은 데이터가 하나 있다고 해보자. 그럼 데이터에서 원을 그려서 가장 가까운 데이터와 같은 성격이라고 지정하는 것이다. 여러 데이터가 원안에 들어오면 그 데이터들의 평균을 내서 정한다.

당연히 문제가 있다. 거리계산을 다해야하기 때문에 속도가 느리다. 개선중이다. 그럼 많이 사용될까?
실제로 성능이 생각보다 많이 괜찮고, 오차율도 생각보다 좋아서 여전히 쓰고있다.

한번 이용해서 진행해보자 BMI 데이터로 사용해보자.

기존의 logistic의 정확도부터 보자.

```python
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/practice/bmi.csv',
                 skiprows=3)

# train validation 분리

x_data, x_val, t_data, t_val = \
train_test_split(df.iloc[:,1:], df.label, 
                 test_size=0.2,
                 random_state=0,
                 stratify=df.label)

# bmi의 데이터는 결측치와 이상치가 없다. 대신 정규화는 해줘야 한다.
scaler = MinMaxScaler()
scaler.fit(x_data)
xn_data = scaler.transform(x_data)
xn_val = scaler.transform(x_val)


model = LogisticRegression()
model.fit(xn_data, t_data)
print(model.score(xn_val, t_val))
------------------------------------------------------------------------------------
0.9845
```

그렇다면 KNN은?

```python
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(xn_data,t_data)
print(knn_model.score(xn_val,t_val))
# k값은 무조건 홀수 쓴다.최근접 이웃 몇개를 가지고 판단하는가이다.
------------------------------------------------------------------------------------
0.99975
```

너무나 잘맞아떨어진다.. 생각보다 성능이 우수하다. 
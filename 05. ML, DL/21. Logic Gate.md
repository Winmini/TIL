# 머신러닝, 딥러닝

## - Logic Gate

딥러닝의 제일 기초가 되는 것은 1960년대에 개발된 퍼셉트론이다. 초기형태의 인공신경망으로 사람의 뉴런을 본 따 만든 알고리즘이다. 이 퍼셉트론으로 논리게이트들을 학습시키고 적용시킬 수 있는지 한번 확인해보자.

AND연산, OR연산, XOR연산에 대해서 해보자. 3개의 논리연산의 입력과 label은 이미 주어져있고, label은 0과 1로 구분되므로  binary logistic regression이다. 텐서플로 1.15버전으로 풀어보자.



#### AND 연산

```python
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report

x_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]], dtype=np.float64)
t_data = np.array([[0], [0], [0], [1]], dtype=np.float64)

X = tf.placeholder(shape=[None,2], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

W = tf.Variable(tf.random.normal([2,1]))
b = tf.Variable(tf.random.normal([1]))

logit = tf.matmul(X,W) + b
H = tf.sigmoid(logit)

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))

train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(300000):
    tmp, loss_val = sess.run([train, loss], feed_dict={X:x_data, T:t_data})
    if not step%30000:
        print('loss : {}'.format(loss_val))
---------------------------------------------------------------------------------------
loss : 0.6657941341400146
loss : 0.05645841360092163
loss : 0.028989799320697784
loss : 0.019381405785679817
loss : 0.014526990242302418
loss : 0.011613089591264725
loss : 0.009671812877058983
loss : 0.008289382793009281
loss : 0.007251552306115627
loss : 0.006443987134844065
```

loss값을 보니 학습을 아주 잘한 것 같다. evaluation을 진행해보자.

```python
acc = tf.cast(H>0.5, dtype=tf.float32)

result = sess.run(acc, feed_dict={X:x_data})
print(result)

print(classification_report(t_data.ravel(), result.ravel()))
---------------------------------------------------------------------------------------
[[0.]
 [0.]
 [0.]
 [1.]]
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00         3
         1.0       1.00      1.00      1.00         1

    accuracy                           1.00         4
   macro avg       1.00      1.00      1.00         4
weighted avg       1.00      1.00      1.00         4
```

결과적으로 AND는 학습이 잘되는 것을 알 수 있다.

OR와 XOR 실습은 출력 t_data값만 바꾸고 바로 진행하면 된다. 해보면 OR연산도 잘맞아 떨어지는 것을 확인할 수 있다.

XOR의 결과를 확인해보면,

```python
loss : 0.9951592683792114
loss : 0.6949375867843628
loss : 0.6931929588317871
loss : 0.6931486129760742
loss : 0.6931472420692444
loss : 0.6931471824645996
loss : 0.6931471824645996
loss : 0.6931471824645996
loss : 0.6931471824645996
loss : 0.6931471824645996
```

loss가 멈춰서 내려가질 않는다. evaluation을 시켜보면

```python
[[1.]
 [1.]
 [1.]
 [0.]]
              precision    recall  f1-score   support

         0.0       1.00      0.50      0.67         2
         1.0       0.67      1.00      0.80         2

    accuracy                           0.75         4
   macro avg       0.83      0.75      0.73         4
weighted avg       0.83      0.75      0.73         4
```

결론적으로 정확도가 0.75로 학습이 되지 않는다. XOR은 Logistic Regression으로 구현할 수 없다.

이를 해결하기 위해 많은 사람들이 노력했지만 실패하여 ai는 상당히 오랜기간 침체기를 겪었다.



결국 multi layer로 해결하게 된다. 이는 딥러닝의 기반이 되는데 1개 이상의 logistic regression을 나타내는 <u>node</u>를 서로 연결시켜서 신경망 구조로 만든 것이다.

일반적으로 입력층, 1개 이상의 은닉층, 출력층으로 구성된다. 출력층에서 나오는 Error를 기반으로 각 node가 가지는 weight를 학습하는 구조이다.

Deep Learning에서는 은닉층을 1개 이상 사용해서 model의 정확도를 높일 수 있다. 한번 구현해서 확인해보자.

먼저 이해를 돕기 위해 tensorflow 1버전을 이용해 구현해보자.

```python
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report

x_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]], dtype=np.float64)
# AND Gate
t_data = np.array([[0], [1], [1], [0]], dtype=np.float64)

X = tf.placeholder(shape=[None,2], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

W1 = tf.Variable(tf.random.normal([2,10]))
b1 = tf.Variable(tf.random.normal([10]))
layer1 = tf.sigmoid(tf.matmul(X,W1)+ b1)

W2 = tf.Variable(tf.random.normal([10,8]))
b2 = tf.Variable(tf.random.normal([8]))
layer2 = tf.sigmoid(tf.matmul(layer1,W2)+ b2)

W3 = tf.Variable(tf.random.normal([8,1]))
b3 = tf.Variable(tf.random.normal([1]))

# hypothesis
logit = tf.matmul(layer2, W3) + b3
H = tf.sigmoid(logit)


loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))

train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(30000):
    tmp, loss_val = sess.run([train, loss], feed_dict={X:x_data, T:t_data})
    if not step%3000:
        print('loss : {}'.format(loss_val))
-------------------------------------------------------------------------------
loss : 0.7053792476654053
loss : 0.6758202314376831
loss : 0.6462671160697937
loss : 0.5865578055381775
loss : 0.5009180903434753
loss : 0.38741469383239746
loss : 0.23957297205924988
loss : 0.13947643339633942
loss : 0.09064489603042603
loss : 0.06530805677175522
```

loss값이 계속 줄어드는 형태를 볼 수 있다.

```python
acc = tf.cast(H>0.5, dtype=tf.float32)

result = sess.run(acc, feed_dict={X:x_data})
print(result)

print(classification_report(t_data.ravel(), result.ravel()))
-------------------------------------------------------------------------------
[[0.]
 [1.]
 [1.]
 [0.]]
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00         2
         1.0       1.00      1.00      1.00         2

    accuracy                           1.00         4
   macro avg       1.00      1.00      1.00         4
weighted avg       1.00      1.00      1.00         4
```

결과도 괜찮다.

이제 tensorflow 2버전을 사용해서 구현해보자. 훨씬 쉽다.

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report

# Training Data Set (XOR)
x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float64)
t_data = np.array([[0], [1], [1], [0]], dtype=np.float64)

# model
model = Sequential()

# layer추가
model.add(Flatten(input_shape=(2,)))

model.add(Dense(10, activation='sigmoid'))
model.add(Dense(8, activation='sigmoid'))
model.add(Dense(20, activation='sigmoid'))

model.add(Dense(1, activation='sigmoid'))

# compile
model.compile(optimizer=Adam(learning_rate=1e-2),
              loss='binary_crossentropy',
              metrics=['acc'])

  # 학습
model.fit(x_data,
          t_data,
          epochs=500,
          verbose=1)

predict_val = model.predict(x_data)

result = tf.cast(predict_val > 0.5, dtype=tf.float32).numpy().ravel()
print(classification_report(t_data.ravel(), result))
------------------------------------------------------------------------------
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00         2
         1.0       1.00      1.00      1.00         2

    accuracy                           1.00         4
   macro avg       1.00      1.00      1.00         4
weighted avg       1.00      1.00      1.00         4
```

잘나오는 것을 확인할 수 있다.
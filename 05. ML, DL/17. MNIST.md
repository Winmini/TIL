# 머신러닝, 딥러닝

## - MNIST

파이썬을 처음 배울 때 `print('Hello World')`하던 것이 기억나는가? 머신러닝도 마찬가지로 그런느낌으로 있는 것이 MNIST이다. 물론 Hello World의 수준으로 쉬운 것은 아니지만 꼭 지나야하는 관문이다. 

MNIST에는 숫자데이터셋이 있다. 6만명의 사람들의 손글씨를 입력해놓은 데이터셋들이다. 이 데이터셋을 왜 만들었을까? 미국에서 우편번호를 인식하기 위해 시작되었다. 매번 손으로 쓰여진 우편번호를 보고 머신러닝을 돌려서 해결하자였다.

숫자데이터셋은 0~9까지 클래스가 10개이므로 Multinomial Classification이고, 10개의 값이 나와야 할 것이다.



분석을 위해 kaggle에서 [데이터](https://www.kaggle.com/c/digit-recognizer)를 가져오자.

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
# 여기까지는 아는 모듈들

# data handling
df = pd.read_csv('./data/mnist/train.csv')


# feature engineering
# 이 예제에서는 버릴데이터도 없고 새로운컬럼을 만들 필요도 없다.

# 결측치 이상치처리
# 매번 해야하지만 여기서는 둘 다 없다.

# 픽셀 데이터를 실제로 그려서 눈으로 확인해보자.
img_data = df.iloc[:,1:].values
# display(img_data)

fig = plt.figure()
fig_arr = []
# 이 안에 subplot 저장할 것임.

for i in range(10):
    fig_arr.append(fig.add_subplot(2,5,i+1))
    fig_arr[i].imshow(img_data[i].reshape(28,28), cmap='Greys', interpolation='nearest')
    # interpolation, 보간, 자연스럽게 이미지 출력
    
plt.tight_layout()
plt.show()
```

`plt.show()`를 통해 이미지데이터가 어떻게 생겼는지도 확인해보자.

![숫자데이터](./jpgfile/숫자데이터.png)

나머지는 늘 하던 작업이다.

```python
# 정규화
scaler= MinMaxScaler()
scaler.fit(df.iloc[:,1:])
df.iloc[:,1:] = scaler.transform(df.iloc[:,1:])

# One-Hot-Encoding
sess = tf.Session()
n_label = sess.run(tf.one_hot(df.label, depth=10))

x_data, x_val, t_data, t_val = \
train_test_split(df.iloc[:,1:], n_label,
                 test_size=0.2,
                 random_state=1,
                 stratify=df.label)

X = tf.placeholder(shape=[None,784], dtype=tf.float32)
T = tf.placeholder(shape=[None,10], dtype=tf.float32)

W = tf.Variable(tf.random.normal([784,10]))
b = tf.Variable(tf.random.normal([10]))

# Hypothesis
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

sess.run(tf.global_variables_initializer())

# 이 방법은 문제가 있다.
for step in range(1000):
    tmp, loss_val = sess.run([train, loss], feed_dict={X:x_data, T:t_data})
    if not step%100:
        print("loss: {}".format(loss_val))
```

지금까지는 이렇게 했었지만 사실 이 방법에는 문제가 있다. x_data의 shape이 (33600,784)이다. 이 말은한번에 그래프로 입력되는 데이터가 33600개라는 소리이고 <u>이번엔</u> 가능했다.

그런데 데이터가 더 많아져서 100만개쯤 된다면 메모리에 한꺼번에 이만큼을 적재할 수 있을까..?
아마 out of memory오류가 나버릴 것이다. 따라서 배치로 나눠서 학습할 것이다. 즉 33600개를 100개씩 336번 학습하는 것이다. 코드를 다음과 같이 수정해보자.

```python
epoch = 3000
# 1 epoch (에폭): training data 전체를 이용해 1번학습을 진행
batch_size = 100
# 100개씩 잘라서 진행
num_of_iter = x_data.shape[0] / batch_size

for step in range(epoch):
    for i in range(int(num_of_iter)):
        batch_x = x_data[i*batch_size:(i+1)*batch_size]
        batch_t = t_data[i*batch_size:(i+1)*batch_size]
        tmp, loss_val = sess.run([train, loss], feed_dict={X:batch_x, T:batch_t})
    if not step % 300:
        print("loss: {}".format(loss_val))
-----------------------------------------------------------------------------------
loss: 0.06159188598394394
loss: 0.04217331483960152
loss: 0.039455000311136246
loss: 0.038720592856407166
loss: 0.03838087245821953
...
```

모델이 완성되었으니 당연히 evaluation을 진행해야 한다.

```python
# H => [0.1 0.2 0.5 ... 0.01] 과 같은 데이터이다
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(T,1))
# true false로 되어있는 것
acc = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
# 실수로 바꿔서 평균내면..

# 2가지 경우로 평가를 진행 - 오버피팅이 발생했나?
# 1. training data로 accuracy 계산 -> 당연히 높아야함.
# 2. validation data로 accuracy를 계산 -> 상대적으로 낮다.
# 만약 이차이가 크다면 오버피팅이 발생한 것이다.

# 1
train_result = sess.run(acc, feed_dict={X:x_data, T:t_data})

# 2
val_result = sess.run(acc, feed_dict={X:x_val, T:t_val})

print('train 정확도: {}, val 정확도: {}'.format(train_result, val_result))
-----------------------------------------------------------------------------------
train 정확도: 0.9944047331809998, val 정확도: 0.9910714030265808
```

sklearn이 제공하는 평가 관련 module을 사용할 수도 있다. 이미 onehot으로 만들었으니 다음에 쓰도록하자..
`sklearn.metrics.classification_report(y_true, y_pred, target_names=[])`

- y_true: 우리가 가지고 있는 정답, 1차원 onehot형태가 아닌 label형태로 입력
- y_pred: 우리가 예측한 모델값, 1차원 이역시 onehot형태가 아닌 label형태
- target_names: label을 효편하기 위해 사용
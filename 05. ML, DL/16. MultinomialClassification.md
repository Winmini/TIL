# 머신러닝, 딥러닝

## - Multinomial Classification

지금까지는 2진분류를 했었다.

데이터묶음 하나와 가중치 한묶음으로 계산을 하였는데, 그게 하나의 선이나왔고 그 선으로 데이터를 구분하거나, 적합하게 하거나만들었다. 선이 하나에 2개를 구분한다면 데이터 3개, 혹은 그이상의 데이터는 구분하기 힘들 것이다. 따라서 가중치를 여러묶음으로 하여 선을 여러개 만든다면 데이터를 여러개 구분할 수 있을 것이다.

![여러개분류](./jpgfile/멀티분류.png)

다음과 같이 말이다. 그렇게 구하고 예측을 하면 결과가 (a b c)의 결과로 떨어질 것이다. 기존 시그모이드함수대신 Soft max함수와 cross entropy(명칭은 같지만 다른식이기는 하다)까지 하고나면 a + b+ c = 1이되고 각각이 A, B, C일 확률로 나온다.

그리고 마지막으로 Label 처리를 해야 한다. 원래 로지스틱은 0아니면 1이지만 멀티노미얼이므로 0과 1이 아닌 문자를 써서 비교한다. 학점이 a, b, c라던지 1등급 2등급 3등급이라던지 그래서 이 부분을 one-hot Encoding을 사용하여 분류한다. 예를 들어 A,B,C을 [1,0,0], [0,1,0], [0,0,1] 각각에 매칭시키는 것이다.



---

### 실습

첨부된 bmi 데이터를 가지고 실습해보자.

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from scipy import stats

df = pd.read_csv('./data/bmi/bmi.csv', skiprows=3)
# 맨위 3줄은 빼고 읽어들이자.
display(df)

# 처음은 당연히 결측치 처리이다.
df.isnull().sum()
# 없다.

# 이상치 처리
zscore_threshold=1.8
np.abs(stats.zscore(df['weight'])) > zscore_threshold
np.abs(stats.zscore(df['height'])) > zscore_threshold
# 이상치가 없다.
# 각 컬럼별로 확인하고 한꺼번에 처리하는 방법은 생각해보자..

# 키와 무게의 정규화
scaler = MinMaxScaler()
scaler.fit(df.iloc[:,1:])
norm_x = scaler.transform(df.iloc[:,1:])

# One-Hot Encoding
# 0 -> [1, 0, 0]
# 1 -> [0, 1, 0]
# 2 -> [0, 0, 1]
# numpy가 가지는 방법, sklearn기능을 이용, tensorflow기능을 이용
# tensorflow 기능을 이용하자.
sess = tf.Session()
hot_t = sess.run(tf.one_hot(df.label,depth=3))
# 클래스가 몇개인지 알려줘야한다. -depth
# 3차원 데이터가 만들어진것 주의하자.

# 데이터 분할
train_x, val_x, train_t, val_t = \
train_test_split(norm_x,
                 hot_t,
                 test_size=0.2,
                 stratify=df.label,
                 random_state=1)

display(train_x)
display(val_t)

X = tf.placeholder(shape=[None,2], dtype=tf.float32)
T = tf.placeholder(shape=[None,3], dtype=tf.float32) # onehot이므로 컬럼 3

W = tf.Variable(tf.random.normal([2,3]))
b = tf.Variable(tf.random.normal([3]))

# Hypothesis(Model) - multinomial classification
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# loss - Binary
# loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
#                                                               labels=T)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))

# train, gadient descent
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

# Session 초기화
sess.run(tf.global_variables_initializer())

for i in range(10000):
    tmp, loss_val = sess.run([train, loss], feed_dict={X:train_x, T:train_t})
    if not i%1000:
        print('loss: {}'.format(loss_val))
-----------------------------------------------------------------------------------
loss: 0.13432040810585022
loss: 0.13341031968593597
loss: 0.13252517580986023
loss: 0.1316634565591812
loss: 0.1308242827653885
loss: 0.13000650703907013
loss: 0.12920944392681122
loss: 0.12843261659145355
loss: 0.12767460942268372
loss: 0.12693437933921814
```

모델을 평가해보자.

```python
# Evaluation 모델 평가.
# binary classification과는 좀 다르다.
# H => [0.6 0.3 0.1]
# T => [1   0   0  ]
# 이 2개를 비교해보자.
# H를 argmax하면 된다.
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(T,1))
acc = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

print('acc: {}'.format(sess.run(acc, feed_dict={X:val_x, T:val_t})))
-----------------------------------------------------------------------------------
acc: 0.9857500195503235
```

예측도 해보자

```python
predict_data=[[187,78]]
predict_data_norm = scaler.transform(predict_data)
result = sess.run(predict, feed_dict={X:predict_data_norm})
print(result)
----------------------------------------------------------------------------------
[1]
```


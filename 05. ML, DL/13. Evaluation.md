# 머신러닝, 딥러닝

## - Evaluation

우리의 모델이 잘만들어졌는지 어떻게 알까? 다행히도 로지스틱회귀는 가능하다. 우리의 Model이 예측한 결과와 실제 정답을 비교해서 정답률을 측정해보면 된다. 평가지표는 다음과 같다

![평가지표](./jpgfile/평가지표.png)



그리고 약어로 표현하여

![지표](./jpgfile/지표.png)

이렇게 약어를 이용한다.

#### 평가지표

- Precision: <u>정밀도</u>라고 부르며, TP / (TP+FP) 이다. 내가 True라고 예측한 것들중에서 맞은것의 개수이다.

- <u>Hit rate</u>, Recall: <u>재현율</u>이라고 부르며, TP / (TP+FN) 이다. 실제 True인것들 중 내가 True라고 입력하여 맞은 개수이다. 보통 정밀도와 Trade OFF관계에 있다.

- Accuracy: <u>정확도</u>라고 부르며, TP + TN / (TP + FN + FP + TN) 전체 데이터중 내가 맞은 것의 개수이다. 일반적으로 정확도를 많이 사용한다. 이게 너무 직관적이기도 하고 이걸로 측정하는게 당연한거아니야? 라고 생각할 수 있지만 이게 가지는 가장 큰 문제점이 하나 있다. Domain(내가 가지고 있는 데이터) 의 bias에 상당히 민감하다는 특징이 있다. 즉, 예를 들어 대부분의 값이 1이거나 대부분의 값이 0인 편향되어 있는 데이터에서 문제가 생긴다. 문제는 골고루 분포되어있는 데이터가 많지 않다는 것이다. 대부분은 치우쳐져있다. 불량품여부, 암검사 등등.. 다 치우쳐져있다. 그럼 이럴때 문제가 어떻게 생길까? 불량을 검사해야 하는데 내가 모델을 만들지 않고 무조건 정상이라고 판별하기만 해도 정확도가 99프로가 될 것이기 때문이다.

  결과적으로 Acuuracy를 지표로 삼으려면 데이터의 편차를 고려해줘야 한다.

- F1 score: Percsion과 Recall의 조화평균, 두 값이 서로 Trade off관계에 있으므로 이 역시 유용하다. 상대적으로 높은 정확도의 영향력을 낮춰서 계산하는 방법이다.



### 추가적으로 생각해야할 문제

- Learning rate: customizing 해야하는 값이다. loss값보고 결정한다. 너무 높게 잡으면 'over shooting'이 일어날 수 있으며, 너무 낮게 잡으면 'local minima'가 발생한다.
- Normalization: 사이킷런이 아니라면 무조건적으로 해야한다. 아니면 제대로 된 결과가 나올 수 없으니 꼭 해야하는 작업이다.
- 과적합
  - Over fitting: 트레이닝 데이터셋에 대해 너무나 잘맞게 만들어진 경우
  - Under fitting: 모델이 완성되지 않은 경우, 보통 학습량자체가 부족함 자주 발생하지 않음


- weight의 값이 크면 클수록 좋지 않다. => 'Regularization'
- Oversampling: 데이터의 불균형, 1이 너무 많고 0이 너무 적고,, 등등 이를 해결하기 위해서 <u>SMOTE</u> 알고리즘을 사용한다. 참고로 언더샘플링이 있긴하지만 언더샘플링하면 전체 트레이닝 데이터셋의 개수가 줄어들어 오버피팅이 다시 발생하는 악순환에 빠진다. 따라서 언더샘플링은 거의 하지 않게 된다. 
- Evaluation(모델 평가): Model을 구현한 후 당연히 성능평가를 진행
  - Tranining Data Set으로 학습한 후, 이 Traning Data Set을 다시 이용해서 성능 평가를 진행하면 당연히 안된다.

------

평가하는 방법

![방법](./jpgfile/kfold.png)

단점은 시간이 오래걸리지만, 매우 좋은 효율을 가지고 있다. 데이터 편향을 방지해준다.

------

**오버피팅이 왜 좋지 않은가?**

![일반적인](./jpgfile/일반적인케이스.png)![과대적합케이스](./jpgfile/과대적합케이스.png)

오른쪽이 과대적합된 case이다. 우리는 일반적으로 왼쪽처럼 학습되길 원한다. 따라서 오버피팅은 좋지 않은것이다.

그렇다면 오버피팅을 피하기 위해서는?

- Data Set이 많아야 한다. 하지만 이건 현실적으로 힘든 경우가 많다.
- 그래서 feature의 개수를 줄여야 한다. (독립변수의 개수) 다 의미가 있을거같으면 줄일 수 없다. 이것도 잘 줄여야 하는데 비슷비슷하거나 의미가 크게 없을 것 같은 값들을 줄이는 것이다.


---

### 실습

위스콘신 유방암 데이터셋을 이용한 Logistic Regression 구현을 해보자. 이 데이터셋은 싸이킷런에 있다.

유방암 데이터에는 유방암 세포의 특징들이 있고, 세포의 평균크기, 평균오차, 등등 30가지 feature가 있다. anaconda로 가서 해보자. 

싸이킷런을 이용한 실습을 먼저 해보자.

```python
from sklearn.datasets import load_breast_cancer
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.model_selection import cross_val_score # K-Fold

# Load Data Set
x_data = load_breast_cancer().data
t_data = load_breast_cancer().target
print(x_data.shape)
print(t_data.shape)

np.unique(t_data, return_counts=True)
# 0과 1만 있는지, 편향된 데이터가 있는지

print(load_breast_cancer().DESCR)
# 데이터에 대한 설명도 확인 가능

train_x_data, validation_x_data, train_t_data, validation_t_data = \
train_test_split(x_data, t_data,
                 test_size=0.2,
                 random_state=1,
                 stratify=t_data)
# random_state = 1 앞에서부터 자르는게 아니라 랜덤하게 자름
# stratify = t_data, 0과 1의 데이터가 골고루 분포되도록 자름

print(train_x_data.shape)
print(validation_x_data.shape)

model = linear_model.LogisticRegression(max_iter=5000)
# iter값으로 반복횟수를 조절할 수는 있다.

# K-fold cross validation
# score = cross_val_score(model,
#                 train_x_data,
#                 train_t_data,
#                 cv=5) # 5-ford
# print(score.mean())
# 0.9472527472527472
# 이런 방법도 있다 하지만 다르게 하자.

model.fit(train_x_data, train_t_data)
acc = model.score(validation_x_data, validation_t_data)
print(acc)
# 0.956140350877193
-------------------------------------------------------------------------------
(455, 30)
(114, 30)
0.956140350877193
```

tensorflow를 이용하여 해보자.

```python
import numpy as np
import tensorflow as tf
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# data loading
cancer = load_breast_cancer()
x_data = cancer.data
t_data = cancer.target

train_x, val_x, train_t, val_t = \
train_test_split(x_data,
                 t_data,
                 test_size=0.2,
                 stratify=t_data,
                 random_state=2)

# Tensorflow 구현(v1.15)

X = tf.placeholder(shape=[None,30], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

W = tf.Variable(tf.random.normal([30,1]))
b = tf.Variable(tf.random.normal([1]))

# Hypothesis
logit = tf.matmul(X,W) + b
H = tf.sigmoid(logit)

# Loss function
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))
# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(300000):
    tmp, loss_val = sess.run([train,loss], 
                         feed_dict={X:train_x, T:train_t.reshape(-1,1)})
    if not step % 30000:
        print('loss: {}'.format(loss_val))
-------------------------------------------------------------------------------
loss: 348.7813415527344
loss: 0.42108672857284546
loss: 0.3974219262599945
loss: 0.3775393068790436
loss: 0.36085885763168335
loss: 0.3466285169124603
loss: 0.33430951833724976
loss: 0.3237113654613495
loss: 0.31460288166999817
loss: 0.18488523364067078
```

학습이 종료되면, 우리 모델이 잘 만들어진 모델인지 확인을 해야한다. 평가지표의 여러지표로 내 데이터가 어느 평가방법이 맞는지를 확인하고 그 것을 바탕으로 평가해야한다.

하지만 계속 실습하는 데이터는 편향된 label데이터를 사용하지 <u>않을</u> 것이기 때문에 Accuracy를 사용할 예정이다.

```python
predict = tf.cast(H > 0.5, dtype=tf.float32)
# 만약 연산결과가 True면, 1.0으로 해줌
correct = tf.equal(predict, T)
acc = tf.reduce_mean(tf.cast(correct,tf.float32))
# 얘도 node임. 실행시켜야함.

acc_val = sess.run(acc, feed_dict={X:val_x, T:val_t.reshape(-1,1)})
print(acc_val)
'''
predict     = [0, 0, 1, 1, 1, 0, 1]
T           = [0, 0, 1, 0 ,1, 0, 0]
correct     = [T, T, T, F, T, T, F]
correct_val = [1, 1, 1, 0, 1, 1, 0]
correct_val의 평균 = 5/7 = 0.71 (71%)
'''
-------------------------------------------------------------------------------
0.9298246
```


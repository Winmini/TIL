# 머신러닝, 딥러닝

## - CNN



- FC Layer: DNN에서 Layer의 여러 노드들이 서로 전부 연결되어 있는 layer이다. 완전연결이 되어있고, 지금까지 했던 모든 layer는 다 FC Layer이다. 다른 말로 Dense Layer라고도 한다.





### Convolution

합성곱연산은, 두 함수 A와 B가 있다면, 하나의 함수를 Reverse시키고 Shift시키면서 다른 함수와 곱해서 나온 결과를 계속 적분하여 얻은 결과이다. 하지만 이렇게는 이해하기 힘들다. 다음을 보자.

보통 이미지연산에 있어서 합성곱을 하기 위해 두 함수는 두 data를 의미한다. 하나의 데이터는 image data, 합성곱할 나머지 데이터는 filter data이다. 이 filter는 정방향(nxn)으로 잡는 것이 좋고 크지 않게 설정하는 것이 좋다. 그래야 특징을 잡기 좋다.

![필터](./jpgfile/필터.png)

그리고 이렇게 각자에 해당하는 위치끼리 곱해서 더한 결과를 구한다. 이렇게 cnn계산을 한다. 이 필터를 지정된 간격으로 이동시키면서 값을 뽑아낸다.  여기서 지정된 간격이란 우리가 정하는 값이고 이 값을 스트라이드라고 한다. 스트라이드가 1이라면 다음의 결과를 얻는다.

![스트라이드](./jpgfile/스트라이드.png)

그리고 다시 합성곱 연산을 진행한다. 이렇게 하면서 오른쪽을 다하면, 아래쪽으로도 스트라이드 값만큼 이동하여 또다시 합성곱을 전부다 진행한다. 그렇게 얻은 <u>값의 모음</u>을 다시 행렬로 만들 수 있다.

그러면 그 값들은 이 데이터에서는 3x3의 행렬이 될 것이다.

이런식으로 계속 계산하면서 필터자체를 업데이트하면 나중에는 특징을 잘뽑아내는 필터로 바뀐다.

일반적으로 3x3, 4x4필터를 많이 사용한다. 

한번 필터를 이용해서 계산과정을 텐서플로우 1.15버전을 통해 확인해보자.

```python
import numpy as np
import tensorflow as tf

image = np.array([[[[1], [2], [3]],
                   [[4], [5], [6]],
                   [[7], [8], [9]]]], dtype=np.float32)
# 1 3 3 1
# 흑백컬러의 이미지형태
# 이미지개수, height, width, channel

weight = np.array([[[[1]],
                   [[2]]],
                   [[[1]],
                   [[2]]]], dtype=np.float32)
# filter의 형태
# height, width, filter channel, filter의 개수
# 고정되어있는 형태임. 텐서플로우의 api가 이렇게 되어있다그냥.
# 2 2 1 1

conv2d = tf.nn.conv2d(image,
                      weight,
                      strides=[1,1,1,1],
                      padding='VALID')
# 패딩을 안쓰겠다. VALID
# 패딩을 쓰겠다.SHAME

sess = tf.Session()
result = sess.run(conv2d)
print(result)
--------------------------------------------------------------------------------
[[[[19.]
   [25.]]

  [[37.]
   [43.]]]]
```

숫자는 아무런 의미가 없다. 다만 필터와 이미지의 모양이 어떻게 생겼는지와 계산이 잘적용이 되는지를 확인하였다.

실제 이미지를 가지고 실습해보자. girl-teddy사진을 사용할 수 있는 경로에 넣고  다음을 진행해보자.

---

### 실습

```python
import matplotlib.pyplot as plt
import matplotlib.image as img

fig = plt.figure()
# 도화지 생성

ax1 = fig.add_subplot(1,2,1)  # 1행 2열의 첫번째
ax2 = fig.add_subplot(1,2,2)  # 1행 2열의 두번째

ori_image = img.imread('./images/girl-teddy.jpg') # 이미지 객체 생성
ax1.imshow(ori_image)

print(ori_image.shape)
# 모양도 살펴보자.
---------------------------------------------------------------------------------
(429, 640, 3)
# 3채널이라는 것을 보면 RGB값이 동일한 값이라고 생각할 수 있다.
```

4차원 DATA여야지 conv연산을 수행할 수 있었다. 따라서 데이터를 (1, 429, 640, 3)으로 변경해야 바꿀 수 있다. 차원을 하나 높이기 위해 다음을 수행하면 된다.

```python
input_image = ori_image.reshape(1,429,640,3)
print(input_image.shape)
---------------------------------------------------------------------------------
(1, 429, 640, 3)
```

우리가 가지고 있는 사진은 흑백이지만 3채널로 되어 있다. 그리고 보통의 컬러이미지도 대부분 원채널로 바꾼다. 색으로 학습하는거보다 모양으로 학습하길 원하는 경우가 그렇기 때문이며, 컬러로 연산하면 연산량이 3배이기에 흑백으로 처리하는일이 많다.

```python
channel_1_image = input_image[:,:,:,0:1].astype(np.float32)
# shape을 변경하지 않으려고 0:1의 슬라이싱을 사용한다.
weight = np.array([[[[-1]],[[0]],[[1]]],
                   [[[-1]],[[0]],[[1]]],
                   [[[-1]],[[0]],[[1]]]], dtype=np.float32)
# height, width, channel, filter개수
# 3 3 1 1
conv2d = tf.nn.conv2d(channel_1_image,
                      weight,
                      strides=[1,1,1,1],
                      padding='VALID')
sess = tf.Session()
result = sess.run(conv2d)

print(result.shape)
# 1, 427, 638, 1
# 패딩을 하지 않았으므로 사이즈가 줄어든게 맞다.
t_img = result[0,:,:,:]
ax2.imshow(t_img)
```

![비교](./jpgfile/필터링.png)

오른쪽으로 특징을 추출해놓았다. 사람에 대한 윤곽과 곰의 윤곽을 뽑아놓았고, 실제로는 이것을 학습시키는 것이다



---

### Pooling

filter의 size와 stride의 크기에 따라 feature map의 크기는 줄어든다. 하지만 패딩을 하면 사이즈조차 줄어들지 않고 filter를 여러개 사용하기 때문에 전체 데이터는 무조건적으로 증가할 수 밖에 없다. 나중에보면 알겠지만 conv층은 1개만 통과하는게 아니다. conv층은 여러개를 사용하는 경우가 많은데 conv층이 지나갈 수록 데이터가 커진다면 연산량은 너무 많아진다. 학습에 너무 오랜시간이 걸리기에, 이를 줄이는 방법중에 하나이다. 또한 크기도 줄이고, 특징도 돋보이게 하는 방법으로 pooling이 있다. 이 pooling방법도 min, average, max방법이 있고 주로 MAX pooling을 선호한다. 그리고 여기서도 kernel과 stride가 있고, kernel은 2x2, stride=2라고 해보자. <u>conv연산이 끝난 후</u>, feature map에서 pooling처리하는 과정은 다음과 같다.

![풀링](./jpgfile/pooling1.png) ![풀링2](./jpgfile/pooling2.png)

이렇게 2x2커널에서 max pooling하면 큰값을 추출하며, stride값이 2이므로 kernel을 그만큼 이동하여 큰값을 추출한다. 값이 크다는 것은 강한 특징을 지칭하는 것이므로 보통 max pooling을 쓴다.

데이터가 너무 많아지는 것을 방지하는것이 풀링작업이기에 데이터가 많아질거 같지 않으면 굳이 하지 않아도 상관없다.

실습해보자. 위에 result파일을 이어서 사용한다.

```python
pooling = tf.nn.max_pool(result,
                         ksize=[1,2,2,1],
                         strides=[1,2,2,1],
                         padding='VALID')
# 4차원을 만들기위해 1,2,2,1이지만 중요한 것은 2x2이다.
# 앞뒤는 무조건 1, 1이다.
# 스트라이드는 무조건 커널사이즈로 맞춰야 한다.
# 그외 이미지 사이즈랑 안맞는건 알아서 처리해준다.
# width나 height가 홀수사이즈라서 스트라이드가 맞지 않아도 된다는 뜻이다.

pooling_result = sess.run(pooling)
print(pooling_result.shape)
pool_img = pooling_result[0,:,:,:]
ax3.imshow(pool_img)
# 이미지를 추가해서 보자.
---------------------------------------------------------------------------------
(1, 213, 319, 1)
# 사이즈가 확 줄었다.
```

![풀링결과](./jpgfile/풀링결과.png)

---

### 실습

MNIST DATA를 이용해서 Relu처리와 filter의 개수도 늘려보자.

```python
# 입력 이미지 형태를
# (28, 28)에서 (1, 28, 28, 1)로 바꿔야 한다.
# 이미지개수, 세로, 가로, 채널수
input_img = ori_img.reshape(1,28,28,1).astype(np.float32)
print(input_img.shape)

weight = np.random.rand(3,3,1,4)

sess = tf.Session()
conv2d = tf.nn.conv2d(input_img,
                      weight,
                      strides=[1,1,1,1],
                      padding='VALID')

conv_result = sess.run(conv2d)

relu = tf.nn.relu(conv2d_result)
relu_result = sess.run(relu)


pooling = tf.nn.max_pool(relu_result,
                         ksize=[1,2,2,1],
                         strides=[1,2,2,1],
                         padding='VALID')

pool_result = sess.run(pooling)
print(pool_result.shape)
i = np.swapaxes(pool_result,0,3)
# 축 교체


for filter_idx, t_img in enumerate(i):
    ax[filter_idx+1].imshow(t_img.squeeze(), cmap='gray')

fig.tight_layout()
plt.show()     
```

![결과](./jpgfile/필터여러개.png)

여기까지 relu까지 처리한 작업이다. 필터도 다 다른 필터이며 자세히 보면 그림의 결과가 다르다.

